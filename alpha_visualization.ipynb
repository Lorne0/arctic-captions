{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Visualizing the model  \n",
    "This notebook is meant as an example for how to create visualizations\n",
    "like the ones provided in the appendix.\n",
    "\n",
    "It is expected that this might need some slight modification depending on the user's \n",
    "setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "from theano import tensor\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pkl\n",
    "import numpy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import skimage\n",
    "import skimage.transform\n",
    "import skimage.io\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import capgen\n",
    "import generate_caps as gencaps\n",
    "import flickr8k\n",
    "import flickr30k\n",
    "import coco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model and dataset  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = {'flickr8k': (flickr8k.load_data, flickr8k.prepare_data),\n",
    "             'flickr30k': (flickr30k.load_data, flickr30k.prepare_data),\n",
    "             'coco': (coco.load_data, coco.prepare_data)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: flickr30k\n"
     ]
    }
   ],
   "source": [
    "# location of the model file, the pkl file should be named \"model_name.npz.pkl\"\n",
    "model= 'flickr30k_deterministic_model.npz'\n",
    "# location of the devset split file like the ones in /splits\n",
    "dev_list = './splits/coco_val.txt' \n",
    "image_path = './data/flickr30k/'\n",
    "\n",
    "\n",
    "# load model model_options\n",
    "with open('%s.pkl'%model, 'rb') as f:\n",
    "    options = pkl.load(f)\n",
    "\n",
    "print 'Loading: ' + options['dataset']\n",
    "\n",
    "flist = []\n",
    "with open(dev_list, 'r') as f:\n",
    "    for l in f:\n",
    "        flist.append(l.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep aspect ratio, and center crop\n",
    "def LoadImage(file_name, resize=256, crop=224):\n",
    "  image = Image.open(file_name)\n",
    "  width, height = image.size\n",
    "\n",
    "  if width > height:\n",
    "    width = (width * resize) / height\n",
    "    height = resize\n",
    "  else:\n",
    "    height = (height * resize) / width\n",
    "    width = resize\n",
    "  left = (width  - crop) / 2\n",
    "  top  = (height - crop) / 2\n",
    "  image_resized = image.resize((width, height), Image.BICUBIC).crop((left, top, left + crop, top + crop))\n",
    "  data = numpy.array(image_resized.convert('RGB').getdata()).reshape(crop, crop, 3)\n",
    "  data = data.astype('float32') / 255\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "    load_data, prepare_data = datasets[options['dataset']]\n",
    "\n",
    "    train, valid, test, worddict = load_data(False, True, False)\n",
    "    print 'Data loaded'\n",
    "\n",
    "    word_idict = dict()\n",
    "    for kk, vv in worddict.iteritems():\n",
    "        word_idict[vv] = kk\n",
    "    word_idict[0] = '<eos>'\n",
    "    word_idict[1] = 'UNK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Theano Graph  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building f_init... Done\n"
     ]
    }
   ],
   "source": [
    "    # build the sampling functions and model\n",
    "    trng = RandomStreams(1234)\n",
    "    use_noise = theano.shared(numpy.float32(0.), name='use_noise')\n",
    "\n",
    "    params = capgen.init_params(options)\n",
    "    params = capgen.load_params(model, params)\n",
    "    tparams = capgen.init_tparams(params)\n",
    "\n",
    "    # word index\n",
    "    f_init, f_next = capgen.build_sampler(tparams, options, use_noise, trng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    trng, use_noise, \\\n",
    "          inps, alphas, alphas_samples, \\\n",
    "          cost, opt_outs = \\\n",
    "          capgen.build_model(tparams, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the alphas and selector value [called \\beta in the paper]\n",
    "\n",
    "# create update rules for the stochastic attention\n",
    "hard_attn_updates = []\n",
    "if options['attn_type'] == 'stochastic':\n",
    "    baseline_time = theano.shared(numpy.float32(0.), name='baseline_time')\n",
    "    hard_attn_updates += [(baseline_time, baseline_time * 0.9 + 0.1 * opt_outs['masked_cost'].mean())]\n",
    "    hard_attn_updates += opt_outs['attn_updates']\n",
    "    \n",
    "f_alpha = theano.function(inps, alphas, name='f_alpha', updates=hard_attn_updates)\n",
    "if options['selector']:\n",
    "    f_sels = theano.function(inps, opt_outs['selector'], name='f_sels', updates=hard_attn_updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Caption and Attention Visualization\n",
    "\n",
    "(The next five cells can be run over and over to visualize a random image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: './path_to_coco_dev_image/COCO_val2014_000000285235.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-fbacf095767c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# groundtruth\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# annotations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLoadImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mflist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-1a6589163bdb>\u001b[0m in \u001b[0;36mLoadImage\u001b[1;34m(file_name, resize, crop)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# keep aspect ratio, and center crop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mLoadImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m   \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m   \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/intuinno/anaconda2/lib/python2.7/site-packages/PIL/Image.pyc\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2247\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2248\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2249\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2251\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: './path_to_coco_dev_image/COCO_val2014_000000285235.jpg'"
     ]
    }
   ],
   "source": [
    "idx = numpy.random.randint(0, len(valid[0])) # random image\n",
    "k = 1 # beam width\n",
    "use_gt = False # set to False if you want to use the generated sample\n",
    "gt = valid[0][idx][0] # groundtruth\n",
    "context = numpy.array(valid[1][valid[0][idx][1]].todense()).reshape([14*14, 512]) # annotations\n",
    "img = LoadImage(image_path+flist[valid[0][idx][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not use_gt:\n",
    "    sample, score = capgen.gen_sample(tparams, f_init, f_next, context, \n",
    "                                      options, trng=trng, k=k, maxlen=200, stochastic=False)\n",
    "    sidx = numpy.argmin(score)\n",
    "    caption = sample[sidx][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: A bowl of fruit with a bunch of bananas on it .\n",
      "GT: A plate of fruit that includes bananas , pears and apples .\n"
     ]
    }
   ],
   "source": [
    "# print the generated caption and the ground truth\n",
    "if use_gt:\n",
    "    caption = map(lambda w: worddict[w] if worddict[w] < options['n_words'] else 1, gt.split())\n",
    "words = map(lambda w: word_idict[w] if w in word_idict else '<UNK>', caption)\n",
    "print 'Sample:', ' '.join(words)\n",
    "print 'GT:', gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = f_alpha(numpy.array(caption).reshape(len(caption),1), \n",
    "                numpy.ones((len(caption),1), dtype='float32'), \n",
    "                context.reshape(1,context.shape[0],context.shape[1]))\n",
    "if options['selector']:\n",
    "    sels = f_sels(numpy.array(caption).reshape(len(caption),1), \n",
    "                   numpy.ones((len(caption),1), dtype='float32'), \n",
    "                   context.reshape(1,context.shape[0],context.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the Visualization   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# display the visualization\n",
    "n_words = alpha.shape[0] + 1\n",
    "w = numpy.round(numpy.sqrt(n_words))\n",
    "h = numpy.ceil(numpy.float32(n_words) / w)\n",
    "        \n",
    "plt.subplot(w, h, 1)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "\n",
    "smooth = True\n",
    "\n",
    "for ii in xrange(alpha.shape[0]):\n",
    "    plt.subplot(w, h, ii+2)\n",
    "    lab = words[ii]\n",
    "    if options['selector']:\n",
    "        lab += '(%0.2f)'%sels[ii]\n",
    "    plt.text(0, 1, lab, backgroundcolor='white', fontsize=13)\n",
    "    plt.text(0, 1, lab, color='black', fontsize=13)\n",
    "    plt.imshow(img)\n",
    "    if smooth:\n",
    "        alpha_img = skimage.transform.pyramid_expand(alpha[ii,0,:].reshape(14,14), upscale=16, sigma=20)\n",
    "    else:\n",
    "        alpha_img = skimage.transform.resize(alpha[ii,0,:].reshape(14,14), [img.shape[0], img.shape[1]])\n",
    "    plt.imshow(alpha_img, alpha=0.8)\n",
    "    plt.set_cmap(cm.Greys_r)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
